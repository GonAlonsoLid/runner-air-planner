Metadata-Version: 2.4
Name: runner-air-planner
Version: 0.2.0
Summary: Data pipeline y modelo ML para predecir el mejor momento para correr en Madrid.
License-File: LICENSE
Author: Runner Air Planner Maintainers
Author-email: maintainers@example.com
Requires-Python: >=3.11,<4.0
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Classifier: Programming Language :: Python :: 3.14
Requires-Dist: fastapi (>=0.110,<0.111)
Requires-Dist: numpy (>=2.3,<3.0)
Requires-Dist: pandas (>=2.1,<3.0)
Requires-Dist: pydantic (>=2.5,<3.0)
Requires-Dist: requests (>=2.31,<3.0)
Requires-Dist: scikit-learn (>=1.3,<2.0)
Requires-Dist: uvicorn[standard] (>=0.27,<0.28)
Description-Content-Type: text/markdown

# Runner Air Planner - ML Model Data Pipeline

Pipeline de datos y modelo de Machine Learning que predice el mejor momento para salir a correr en Madrid basÃ¡ndose en la calidad del aire y condiciones meteorolÃ³gicas.

## ðŸŽ¯ Objetivo

Crear un dataset estructurado con **mÃ­nimo 1000 registros** y entrenar un modelo ML que combine:
- **Calidad del aire** por estaciÃ³n (NOâ‚‚, Oâ‚ƒ, PM10, PM2.5, etc.)
- **Condiciones meteorolÃ³gicas** (temperatura, humedad, viento)
- **Features temporales** (hora, dÃ­a semana, mes)
- **Features de sinergia** (interacciones entre variables)

## ðŸš€ Inicio RÃ¡pido con Docker (Recomendado)

### Primera vez

```bash
# Construir y levantar la aplicaciÃ³n
docker-compose up -d --build

# Ver los logs para verificar que todo funciona
docker-compose logs -f
```

### Uso normal

```bash
# Levantar la aplicaciÃ³n
docker-compose up -d

# Ver logs
docker-compose logs -f

# Detener la aplicaciÃ³n
docker-compose down
```

**La aplicaciÃ³n estarÃ¡ disponible en:**
- **Frontend**: http://localhost:8080
- **API**: http://localhost:8001
- **API Health Check**: http://localhost:8001/api/health

### Comandos Ãºtiles con Docker

```bash
# Recopilar datos
docker-compose exec api poetry run collect --accumulate

# Entrenar modelo (cuando tengas 1000+ registros)
docker-compose exec api poetry run train

# Hacer predicciones
docker-compose exec api poetry run predict

# Abrir shell en el contenedor
docker-compose exec api bash

# Detener
docker-compose down
```

## ðŸ“¦ InstalaciÃ³n Local (Sin Docker)

### Requisitos

- Python 3.11 o superior
- Poetry instalado

### Pasos

1. **Instalar Poetry** (si no lo tienes):
```bash
# Windows (PowerShell)
(Invoke-WebRequest -Uri https://install.python-poetry.org -UseBasicParsing).Content | python -

# Linux/Mac
curl -sSL https://install.python-poetry.org | python3 -
```

2. **Instalar dependencias:**
```bash
poetry install
```

3. **Activar el entorno virtual:**
```bash
poetry shell
```

4. **Ejecutar la API:**
```bash
# OpciÃ³n 1: Con uvicorn directamente
uvicorn runner_air_planner.api.main:app --host 0.0.0.0 --port 8001 --reload

# OpciÃ³n 2: Con Python
python -m runner_air_planner.api.main
```

5. **Servir el frontend:**
```bash
# Con Python simple server
cd frontend
python -m http.server 8080

# O con cualquier servidor web estÃ¡tico
# El frontend estÃ¡ en la carpeta frontend/
```

**La aplicaciÃ³n estarÃ¡ disponible en:**
- **Frontend**: http://localhost:8080
- **API**: http://localhost:8001

## ðŸ“Š Uso

### 1. Recopilar datos

```bash
# Con Docker
docker-compose exec api poetry run collect --accumulate

# Localmente
poetry run collect --accumulate
```

### 2. Acumular hasta 1000+ registros

La API de Madrid solo devuelve datos del dÃ­a actual (~300-400 registros). Para alcanzar 1000+:

```bash
# Ejecutar varias veces (cada 1-2 horas)
docker-compose exec api poetry run collect --accumulate

# O automÃ¡tico con el script
python scripts/collect_multiple_days.py --min-records 1000 --interval-hours 1
```

### 3. Entrenar modelo

```bash
# Con Docker
docker-compose exec api poetry run train

# Localmente
poetry run train
```

### 4. Hacer predicciones

```bash
# Con Docker
docker-compose exec api poetry run predict

# Localmente
poetry run predict
```

### 5. Visualizar en Frontend

Abre http://localhost:8080 en tu navegador (si usas Docker) o http://localhost:8080 si ejecutas el servidor localmente.

El frontend se conecta automÃ¡ticamente a la API en http://localhost:8001.

## ðŸ—ï¸ Estructura del Proyecto

```
runner-air-planner/
â”œâ”€â”€ src/runner_air_planner/
â”‚   â”œâ”€â”€ data_pipeline/          # Pipeline de datos para ML
â”‚   â”‚   â”œâ”€â”€ ingest_madrid_air.py    # Descarga datos calidad aire
â”‚   â”‚   â”œâ”€â”€ weather.py              # Cliente Open-Meteo
â”‚   â”‚   â”œâ”€â”€ master_data.py          # Datos maestros
â”‚   â”‚   â”œâ”€â”€ data_collector.py       # Clase principal que integra todo
â”‚   â”‚   â”œâ”€â”€ accumulate_data.py      # AcumulaciÃ³n de datos histÃ³ricos
â”‚   â”‚   â””â”€â”€ cli_collect.py          # CLI para recopilar datos
â”‚   â”œâ”€â”€ ml/                      # Modelos de Machine Learning
â”‚   â”‚   â”œâ”€â”€ model.py                # DefiniciÃ³n del modelo
â”‚   â”‚   â”œâ”€â”€ train.py                # Entrenamiento
â”‚   â”‚   â””â”€â”€ predict.py              # Predicciones
â”‚   â””â”€â”€ api/                      # API Backend
â”‚       â””â”€â”€ main.py                # FastAPI application
â”œâ”€â”€ frontend/                     # Frontend estÃ¡tico (HTML/JS/CSS)
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ app.js
â”‚   â””â”€â”€ styles.css
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ collect_multiple_days.py  # Script para recopilar datos automÃ¡ticamente
â”‚   â””â”€â”€ docker-entrypoint.sh       # Script de entrada para Docker
â”œâ”€â”€ data/                         # Datasets y modelos
â”‚   â”œâ”€â”€ ml_dataset_accumulated.csv
â”‚   â””â”€â”€ models/
â”‚       â””â”€â”€ running_model.pkl
â”œâ”€â”€ Dockerfile                    # Imagen Docker para la API
â”œâ”€â”€ docker-compose.yml            # ConfiguraciÃ³n Docker Compose
â””â”€â”€ pyproject.toml                # GestiÃ³n con Poetry
```

## ðŸ“ˆ Dataset para ML

El dataset final (`ml_dataset_accumulated.csv`) contiene **~42 features**:

- **Contaminantes**: `no2`, `o3`, `pm10`, `pm25`, `no`, `nox`, `so2`, `co`
- **EstaciÃ³n**: cÃ³digo, nombre, tipo (TrÃ¡fico/Suburbana), coordenadas
- **Temporales**: hora, dÃ­a semana, mes, `is_weekend`, `is_rush_hour`
- **MeteorolÃ³gicas**: temperatura, humedad, viento, cÃ³digo tiempo
- **Sinergias**: `wind_*_synergy`, `temp_o3_synergy`, `air_quality_index`, etc.

## ðŸ”Œ APIs Utilizadas

- **Calidad del Aire Madrid**: `https://datos.madrid.es/egob/catalogo/212531-12751102-calidad-aire-tiempo-real.json`
- **Open-Meteo**: `https://api.open-meteo.com/v1/forecast` (gratuita, sin API key)

## ðŸ“‹ Requisitos

- **Docker & Docker Compose** (recomendado)
- O **Python 3.11+** y **Poetry** (para desarrollo local)

## ðŸ“ Notas

- La primera vez que ejecutes la app, necesitarÃ¡s recopilar datos y entrenar el modelo
- Los datos se guardan en `data/ml_dataset_accumulated.csv`
- El modelo entrenado se guarda en `data/models/running_model.pkl`
- Los datos se acumulan automÃ¡ticamente, eliminando duplicados
- Por defecto se mantienen Ãºltimos 30 dÃ­as de historial
- El dataset se actualiza incrementalmente con cada ejecuciÃ³n

## ðŸ”§ API Endpoints

La API FastAPI proporciona los siguientes endpoints:

- `GET /` - Health check bÃ¡sico
- `GET /api/health` - Health check detallado
- `GET /api/data/realtime` - Obtener datos de calidad del aire en tiempo real
- `GET /api/data/historical` - Obtener datos histÃ³ricos acumulados
- `POST /api/predict` - Ejecutar predicciones ML con el modelo entrenado

### Ejemplo de uso de la API

```bash
# Obtener datos en tiempo real
curl http://localhost:8001/api/data/realtime

# Hacer predicciones
curl -X POST http://localhost:8001/api/predict \
  -H "Content-Type: application/json" \
  -d '{"use_realtime": true}'
```

## ðŸ› Troubleshooting

### Docker no inicia
```bash
# Ver logs
docker-compose logs

# Reconstruir imagen
docker-compose build --no-cache
docker-compose up -d
```

### Poetry no encuentra dependencias
```bash
# Reinstalar
poetry install

# Limpiar cache
poetry cache clear pypi --all
poetry install
```

### Puerto ocupado
Si el puerto 8001 o 8080 estÃ¡n ocupados, puedes cambiarlos en `docker-compose.yml`:
```yaml
ports:
  - "8002:8000"  # Cambia 8001 a 8002
```

### El frontend no se conecta a la API
Verifica que:
1. La API estÃ© corriendo en http://localhost:8001
2. El frontend estÃ© accediendo a la URL correcta (ver `frontend/app.js`)

## â˜ï¸ Despliegue en Render con Docker

El proyecto estÃ¡ configurado para desplegarse en Render usando **Docker**.

### ConfiguraciÃ³n en Render

1. **Crea un nuevo servicio Web Service** en Render
2. **Conecta tu repositorio de GitHub**
3. **Configura el servicio**:
   - **Environment**: `Docker` (o "Dockerfile")
   - Render detectarÃ¡ automÃ¡ticamente el `Dockerfile` en la raÃ­z
   - **Health Check Path**: `/api/health` (opcional)

4. **Variables de Entorno** (opcional, en Settings â†’ Environment):
   - `PYTHONUNBUFFERED`: `1`

### CÃ³mo funciona

- Render construye la imagen Docker usando el `Dockerfile`
- El `Dockerfile` instala Poetry y todas las dependencias desde `pyproject.toml`
- La aplicaciÃ³n se inicia automÃ¡ticamente con uvicorn
- Render asigna automÃ¡ticamente el puerto usando la variable `PORT`

### Notas importantes para Render

- âœ… El `Dockerfile` ya estÃ¡ configurado para usar la variable `PORT` de Render
- âœ… No necesitas `requirements.txt` ni `render.yaml` (el Dockerfile usa Poetry directamente)
- âš ï¸ Los datos se almacenan en el sistema de archivos del contenedor (no persisten entre reinicios)
- ðŸ’¡ Para datos persistentes, considera usar un servicio de base de datos o almacenamiento externo
- ðŸŒ El frontend estÃ¡tico necesita desplegarse por separado o integrarse con la API

### SoluciÃ³n de problemas

**El servicio no inicia:**
- Verifica que el tipo de servicio sea **"Web Service"** con **"Docker"** como environment
- Revisa los logs en Render para ver errores especÃ­ficos

**Error de puerto:**
- El `Dockerfile` ya estÃ¡ configurado para usar `${PORT}` automÃ¡ticamente
- No definas la variable `PORT` manualmente en Render

