# Runner Air Planner - ML Model Data Pipeline

Pipeline de datos y modelo de Machine Learning que predice el mejor momento para salir a correr en Madrid basÃ¡ndose en la calidad del aire y condiciones meteorolÃ³gicas.

## ğŸ¯ Objetivo

Crear un dataset estructurado con **mÃ­nimo 1000 registros** y entrenar un modelo ML que combine:
- **Calidad del aire** por estaciÃ³n (NOâ‚‚, Oâ‚ƒ, PM10, PM2.5, etc.)
- **Condiciones meteorolÃ³gicas** (temperatura, humedad, viento)
- **Features temporales** (hora, dÃ­a semana, mes)
- **Features de sinergia** (interacciones entre variables)

## ğŸš€ Inicio RÃ¡pido con Docker (Recomendado)

### Primera vez

```bash
# Construir y levantar la aplicaciÃ³n
docker-compose up -d --build

# Ver los logs para verificar que todo funciona
docker-compose logs -f
```

### Uso normal

```bash
# Levantar la aplicaciÃ³n
docker-compose up -d

# Ver logs
docker-compose logs -f

# Detener la aplicaciÃ³n
docker-compose down
```

**La aplicaciÃ³n estarÃ¡ disponible en:**
- **Frontend + API**: http://localhost:8080
- **API Health Check**: http://localhost:8080/api/health

### Comandos Ãºtiles con Docker

```bash
# Recopilar datos
docker-compose exec api poetry run collect --accumulate

# Entrenar modelo (cuando tengas 1000+ registros)
docker-compose exec api poetry run train

# Hacer predicciones
docker-compose exec api poetry run predict

# Abrir shell en el contenedor
docker-compose exec api bash

# Detener
docker-compose down
```

## ğŸ“¦ InstalaciÃ³n Local (Sin Docker)

### Requisitos

- Python 3.11 o superior
- Poetry instalado

### Pasos

1. **Instalar Poetry** (si no lo tienes):
```bash
# Windows (PowerShell)
(Invoke-WebRequest -Uri https://install.python-poetry.org -UseBasicParsing).Content | python -

# Linux/Mac
curl -sSL https://install.python-poetry.org | python3 -
```

2. **Instalar dependencias:**
```bash
poetry install
```

3. **Activar el entorno virtual:**
```bash
poetry shell
```

4. **Ejecutar la API:**
```bash
# OpciÃ³n 1: Con uvicorn directamente
uvicorn runner_air_planner.api.main:app --host 0.0.0.0 --port 8001 --reload

# OpciÃ³n 2: Con Python
python -m runner_air_planner.api.main
```

5. **Servir el frontend:**
```bash
# Con Python simple server
cd frontend
python -m http.server 8080

# O con cualquier servidor web estÃ¡tico
# El frontend estÃ¡ en la carpeta frontend/
```

**La aplicaciÃ³n estarÃ¡ disponible en:**
- **Frontend + API**: http://localhost:8000 (o el puerto que especifiques)

## ğŸ“Š Uso

### 1. Recopilar datos

```bash
# Con Docker
docker-compose exec api poetry run collect --accumulate

# Localmente
poetry run collect --accumulate
```

### 2. Acumular hasta 1000+ registros

La API de Madrid solo devuelve datos del dÃ­a actual (~300-400 registros). Para alcanzar 1000+:

```bash
# Ejecutar varias veces (cada 1-2 horas)
docker-compose exec api poetry run collect --accumulate

# O automÃ¡tico con el script (corre en background cada 30 min hasta 2000 registros)
PYTHONUNBUFFERED=1 nohup python scripts/collect_multiple_days.py \
  --min-records 2000 \
  --interval-hours 0.5 \
  --max-iterations 200 > data/collection.log 2>&1 &

# Ver progreso
tail -f data/collection.log

# Ver procesos de recolecciÃ³n activos
ps aux | grep collect_multiple_days | grep -v grep
```

### 3. Entrenar modelo

```bash
# Con Docker
docker-compose exec api poetry run train

# Localmente
poetry run train
```

### 4. Hacer predicciones

```bash
# Con Docker
docker-compose exec api poetry run predict

# Localmente
poetry run predict
```

### 5. Visualizar en Frontend

Abre http://localhost:8080 en tu navegador (si usas Docker) o http://localhost:8000 si ejecutas localmente con uvicorn.

El frontend y la API se sirven desde el mismo servidor (FastAPI sirve los archivos estÃ¡ticos).

## ğŸ—ï¸ Estructura del Proyecto

```
runner-air-planner/
â”œâ”€â”€ src/runner_air_planner/
â”‚   â”œâ”€â”€ data_pipeline/          # Pipeline de datos para ML
â”‚   â”‚   â”œâ”€â”€ ingest_madrid_air.py    # Descarga datos calidad aire
â”‚   â”‚   â”œâ”€â”€ weather.py              # Cliente Open-Meteo
â”‚   â”‚   â”œâ”€â”€ master_data.py          # Datos maestros
â”‚   â”‚   â”œâ”€â”€ data_collector.py       # Clase principal que integra todo
â”‚   â”‚   â”œâ”€â”€ accumulate_data.py      # AcumulaciÃ³n de datos histÃ³ricos
â”‚   â”‚   â””â”€â”€ cli_collect.py          # CLI para recopilar datos
â”‚   â”œâ”€â”€ ml/                      # Modelos de Machine Learning
â”‚   â”‚   â”œâ”€â”€ model.py                # DefiniciÃ³n del modelo
â”‚   â”‚   â”œâ”€â”€ train.py                # Entrenamiento
â”‚   â”‚   â””â”€â”€ predict.py              # Predicciones
â”‚   â””â”€â”€ api/                      # API Backend
â”‚       â””â”€â”€ main.py                # FastAPI application
â”œâ”€â”€ frontend/                     # Frontend estÃ¡tico (HTML/JS/CSS)
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ app.js
â”‚   â””â”€â”€ styles.css
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ collect_multiple_days.py  # Script para recopilar datos automÃ¡ticamente
â”œâ”€â”€ data/                         # Datasets y modelos
â”‚   â”œâ”€â”€ ml_dataset_accumulated.csv
â”‚   â””â”€â”€ models/
â”‚       â””â”€â”€ running_model.pkl
â”œâ”€â”€ Dockerfile                    # Imagen Docker para la API
â”œâ”€â”€ docker-compose.yml            # ConfiguraciÃ³n Docker Compose
â”œâ”€â”€ pyproject.toml                # GestiÃ³n con Poetry
â””â”€â”€ .pre-commit-config.yaml       # ConfiguraciÃ³n de pre-commit hooks
```

## ğŸ“ˆ Dataset para ML

El dataset final (`ml_dataset_accumulated.csv`) contiene **~42 features**:

- **Contaminantes**: `no2`, `o3`, `pm10`, `pm25`, `no`, `nox`, `so2`, `co`
- **EstaciÃ³n**: cÃ³digo, nombre, tipo (TrÃ¡fico/Suburbana), coordenadas
- **Temporales**: hora, dÃ­a semana, mes, `is_weekend`, `is_rush_hour`
- **MeteorolÃ³gicas**: temperatura, humedad, viento, cÃ³digo tiempo
- **Sinergias**: `wind_*_synergy`, `temp_o3_synergy`, `air_quality_index`, etc.

## ğŸ”Œ APIs Utilizadas

- **Calidad del Aire Madrid**: `https://datos.madrid.es/egob/catalogo/212531-12751102-calidad-aire-tiempo-real.json`
- **Open-Meteo**: `https://api.open-meteo.com/v1/forecast` (gratuita, sin API key)

## ğŸ“‹ Requisitos

- **Docker & Docker Compose** (recomendado)
- O **Python 3.11+** y **Poetry** (para desarrollo local)

## ğŸ“ Notas

- La primera vez que ejecutes la app, necesitarÃ¡s recopilar datos y entrenar el modelo
- Los datos se guardan en `data/ml_dataset_accumulated.csv`
- El modelo entrenado se guarda en `data/models/running_model.pkl`
- Los datos se acumulan automÃ¡ticamente, eliminando duplicados
- Por defecto se mantienen Ãºltimos 30 dÃ­as de historial
- El dataset se actualiza incrementalmente con cada ejecuciÃ³n

## ğŸ”§ API Endpoints

La API FastAPI proporciona los siguientes endpoints:

- `GET /` - Health check bÃ¡sico
- `GET /api/health` - Health check detallado
- `GET /api/data/realtime` - Obtener datos de calidad del aire en tiempo real
- `GET /api/data/historical` - Obtener datos histÃ³ricos acumulados
- `POST /api/predict` - Ejecutar predicciones ML con el modelo entrenado

### Ejemplo de uso de la API

```bash
# Obtener datos en tiempo real
curl http://localhost:8080/api/data/realtime

# Hacer predicciones
curl -X POST http://localhost:8080/api/predict \
  -H "Content-Type: application/json" \
  -d '{"use_realtime": true}'
```

## ğŸ› Troubleshooting

### Docker no inicia
```bash
# Ver logs
docker-compose logs

# Reconstruir imagen
docker-compose build --no-cache
docker-compose up -d
```

### Poetry no encuentra dependencias
```bash
# Reinstalar
poetry install

# Limpiar cache
poetry cache clear pypi --all
poetry install
```

### Puerto ocupado
Si el puerto 8080 estÃ¡ ocupado, puedes cambiarlo en `docker-compose.yml`:
```yaml
ports:
  - "9000:8000"  # Cambia 8080 a 9000
```

### El frontend no carga
Verifica que:
1. La API estÃ© corriendo (Docker o uvicorn)
2. Limpia el cachÃ© del navegador: `Cmd + Shift + R` (Mac) o `Ctrl + Shift + R` (Windows/Linux)

## ğŸ› ï¸ Desarrollo

### Pre-commit hooks

El proyecto usa pre-commit para mantener la calidad del cÃ³digo:

```bash
# Instalar pre-commit
pip install pre-commit
pre-commit install

# Ejecutar en todos los archivos
pre-commit run --all-files
```

Hooks configurados:
- `trailing-whitespace` - elimina espacios al final
- `end-of-file-fixer` - asegura newline al final
- `check-yaml` - valida archivos YAML
- `ruff` - linting con autofix
- `ruff-format` - formato de cÃ³digo

## â˜ï¸ Despliegue en Render con Docker

El proyecto estÃ¡ configurado para desplegarse en Render usando **Docker**.

### ConfiguraciÃ³n en Render

1. **Crea un nuevo servicio Web Service** en Render
2. **Conecta tu repositorio de GitHub**
3. **Configura el servicio**:
   - **Environment**: `Docker` (o "Dockerfile")
   - Render detectarÃ¡ automÃ¡ticamente el `Dockerfile` en la raÃ­z
   - **Health Check Path**: `/api/health` (opcional)

4. **Variables de Entorno** (opcional, en Settings â†’ Environment):
   - `PYTHONUNBUFFERED`: `1`

### CÃ³mo funciona

- Render construye la imagen Docker usando el `Dockerfile`
- El `Dockerfile` instala Poetry y todas las dependencias desde `pyproject.toml`
- La aplicaciÃ³n se inicia automÃ¡ticamente con uvicorn
- Render asigna automÃ¡ticamente el puerto usando la variable `PORT`

### Notas importantes para Render

- âœ… El `Dockerfile` ya estÃ¡ configurado para usar la variable `PORT` de Render
- âœ… No necesitas `requirements.txt` ni `render.yaml` (el Dockerfile usa Poetry directamente)
- âš ï¸ Los datos se almacenan en el sistema de archivos del contenedor (no persisten entre reinicios)
- ğŸ’¡ Para datos persistentes, considera usar un servicio de base de datos o almacenamiento externo
- ğŸŒ El frontend estÃ¡tico necesita desplegarse por separado o integrarse con la API

### SoluciÃ³n de problemas

**El servicio no inicia:**
- Verifica que el tipo de servicio sea **"Web Service"** con **"Docker"** como environment
- Revisa los logs en Render para ver errores especÃ­ficos

**Error de puerto:**
- El `Dockerfile` ya estÃ¡ configurado para usar `${PORT}` automÃ¡ticamente
- No definas la variable `PORT` manualmente en Render
