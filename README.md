# Runner Air Planner - ML Model Data Pipeline

Pipeline de datos y modelo de Machine Learning que predice el mejor momento para salir a correr en Madrid basÃ¡ndose en la calidad del aire y condiciones meteorolÃ³gicas.

## ğŸ¯ Objetivo

Crear un dataset estructurado con **mÃ­nimo 1000 registros** y entrenar un modelo ML que combine:
- **Calidad del aire** por estaciÃ³n (NOâ‚‚, Oâ‚ƒ, PM10, PM2.5, etc.)
- **Condiciones meteorolÃ³gicas** (temperatura, humedad, viento)
- **Features temporales** (hora, dÃ­a semana, mes)
- **Features de sinergia** (interacciones entre variables)

## ğŸš€ Inicio RÃ¡pido con Docker (Recomendado)

### Primera vez

```bash
# Construir y levantar la aplicaciÃ³n
docker-compose up -d --build

# Ver los logs para verificar que todo funciona
docker-compose logs -f
```

### Uso normal

```bash
# Levantar la aplicaciÃ³n
docker-compose up -d

# Ver logs
docker-compose logs -f

# Detener la aplicaciÃ³n
docker-compose down
```

**La aplicaciÃ³n estarÃ¡ disponible en:**
- **Frontend**: http://localhost:8080
- **API**: http://localhost:8001
- **API Health Check**: http://localhost:8001/api/health

### Comandos Ãºtiles con Docker

```bash
# Recopilar datos
docker-compose exec api poetry run collect --accumulate

# Entrenar modelo (cuando tengas 1000+ registros)
docker-compose exec api poetry run train

# Hacer predicciones
docker-compose exec api poetry run predict

# Abrir shell en el contenedor
docker-compose exec api bash

# Detener
docker-compose down
```

## ğŸ“¦ InstalaciÃ³n Local (Sin Docker)

### Requisitos

- Python 3.11 o superior
- Poetry instalado

### Pasos

1. **Instalar Poetry** (si no lo tienes):
```bash
# Windows (PowerShell)
(Invoke-WebRequest -Uri https://install.python-poetry.org -UseBasicParsing).Content | python -

# Linux/Mac
curl -sSL https://install.python-poetry.org | python3 -
```

2. **Instalar dependencias:**
```bash
poetry install
```

3. **Activar el entorno virtual:**
```bash
poetry shell
```

4. **Ejecutar la API:**
```bash
# OpciÃ³n 1: Con uvicorn directamente
uvicorn runner_air_planner.api.main:app --host 0.0.0.0 --port 8001 --reload

# OpciÃ³n 2: Con Python
python -m runner_air_planner.api.main
```

5. **Servir el frontend:**
```bash
# Con Python simple server
cd frontend
python -m http.server 8080

# O con cualquier servidor web estÃ¡tico
# El frontend estÃ¡ en la carpeta frontend/
```

**La aplicaciÃ³n estarÃ¡ disponible en:**
- **Frontend**: http://localhost:8080
- **API**: http://localhost:8001

## ğŸ“Š Uso

### 1. Recopilar datos

```bash
# Con Docker
docker-compose exec api poetry run collect --accumulate

# Localmente
poetry run collect --accumulate
```

### 2. Acumular hasta 1000+ registros

La API de Madrid solo devuelve datos del dÃ­a actual (~300-400 registros). Para alcanzar 1000+:

```bash
# Ejecutar varias veces (cada 1-2 horas)
docker-compose exec api poetry run collect --accumulate

# O automÃ¡tico con el script
python scripts/collect_multiple_days.py --min-records 1000 --interval-hours 1
```

### 3. Entrenar modelo

```bash
# Con Docker
docker-compose exec api poetry run train

# Localmente
poetry run train
```

### 4. Hacer predicciones

```bash
# Con Docker
docker-compose exec api poetry run predict

# Localmente
poetry run predict
```

### 5. Visualizar en Frontend

Abre http://localhost:8080 en tu navegador (si usas Docker) o http://localhost:8080 si ejecutas el servidor localmente.

El frontend se conecta automÃ¡ticamente a la API en http://localhost:8001.

## ğŸ—ï¸ Estructura del Proyecto

```
runner-air-planner/
â”œâ”€â”€ src/runner_air_planner/
â”‚   â”œâ”€â”€ data_pipeline/          # Pipeline de datos para ML
â”‚   â”‚   â”œâ”€â”€ ingest_madrid_air.py    # Descarga datos calidad aire
â”‚   â”‚   â”œâ”€â”€ weather.py              # Cliente Open-Meteo
â”‚   â”‚   â”œâ”€â”€ master_data.py          # Datos maestros
â”‚   â”‚   â”œâ”€â”€ data_collector.py       # Clase principal que integra todo
â”‚   â”‚   â”œâ”€â”€ accumulate_data.py      # AcumulaciÃ³n de datos histÃ³ricos
â”‚   â”‚   â””â”€â”€ cli_collect.py          # CLI para recopilar datos
â”‚   â”œâ”€â”€ ml/                      # Modelos de Machine Learning
â”‚   â”‚   â”œâ”€â”€ model.py                # DefiniciÃ³n del modelo
â”‚   â”‚   â”œâ”€â”€ train.py                # Entrenamiento
â”‚   â”‚   â””â”€â”€ predict.py              # Predicciones
â”‚   â””â”€â”€ api/                      # API Backend
â”‚       â””â”€â”€ main.py                # FastAPI application
â”œâ”€â”€ frontend/                     # Frontend estÃ¡tico (HTML/JS/CSS)
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ app.js
â”‚   â””â”€â”€ styles.css
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ collect_multiple_days.py  # Script para recopilar datos automÃ¡ticamente
â”‚   â””â”€â”€ docker-entrypoint.sh       # Script de entrada para Docker
â”œâ”€â”€ data/                         # Datasets y modelos
â”‚   â”œâ”€â”€ ml_dataset_accumulated.csv
â”‚   â””â”€â”€ models/
â”‚       â””â”€â”€ running_model.pkl
â”œâ”€â”€ Dockerfile                    # Imagen Docker para la API
â”œâ”€â”€ docker-compose.yml            # ConfiguraciÃ³n Docker Compose
â””â”€â”€ pyproject.toml                # GestiÃ³n con Poetry
```

## ğŸ“ˆ Dataset para ML

El dataset final (`ml_dataset_accumulated.csv`) contiene **~42 features**:

- **Contaminantes**: `no2`, `o3`, `pm10`, `pm25`, `no`, `nox`, `so2`, `co`
- **EstaciÃ³n**: cÃ³digo, nombre, tipo (TrÃ¡fico/Suburbana), coordenadas
- **Temporales**: hora, dÃ­a semana, mes, `is_weekend`, `is_rush_hour`
- **MeteorolÃ³gicas**: temperatura, humedad, viento, cÃ³digo tiempo
- **Sinergias**: `wind_*_synergy`, `temp_o3_synergy`, `air_quality_index`, etc.

## ğŸ”Œ APIs Utilizadas

- **Calidad del Aire Madrid**: `https://datos.madrid.es/egob/catalogo/212531-12751102-calidad-aire-tiempo-real.json`
- **Open-Meteo**: `https://api.open-meteo.com/v1/forecast` (gratuita, sin API key)

## ğŸ“‹ Requisitos

- **Docker & Docker Compose** (recomendado)
- O **Python 3.11+** y **Poetry** (para desarrollo local)

## ğŸ“ Notas

- La primera vez que ejecutes la app, necesitarÃ¡s recopilar datos y entrenar el modelo
- Los datos se guardan en `data/ml_dataset_accumulated.csv`
- El modelo entrenado se guarda en `data/models/running_model.pkl`
- Los datos se acumulan automÃ¡ticamente, eliminando duplicados
- Por defecto se mantienen Ãºltimos 30 dÃ­as de historial
- El dataset se actualiza incrementalmente con cada ejecuciÃ³n

## ğŸ”§ API Endpoints

La API FastAPI proporciona los siguientes endpoints:

- `GET /` - Health check bÃ¡sico
- `GET /api/health` - Health check detallado
- `GET /api/data/realtime` - Obtener datos de calidad del aire en tiempo real
- `GET /api/data/historical` - Obtener datos histÃ³ricos acumulados
- `POST /api/predict` - Ejecutar predicciones ML con el modelo entrenado

### Ejemplo de uso de la API

```bash
# Obtener datos en tiempo real
curl http://localhost:8001/api/data/realtime

# Hacer predicciones
curl -X POST http://localhost:8001/api/predict \
  -H "Content-Type: application/json" \
  -d '{"use_realtime": true}'
```

## ğŸ› Troubleshooting

### Docker no inicia
```bash
# Ver logs
docker-compose logs

# Reconstruir imagen
docker-compose build --no-cache
docker-compose up -d
```

### Poetry no encuentra dependencias
```bash
# Reinstalar
poetry install

# Limpiar cache
poetry cache clear pypi --all
poetry install
```

### Puerto ocupado
Si el puerto 8001 o 8080 estÃ¡n ocupados, puedes cambiarlos en `docker-compose.yml`:
```yaml
ports:
  - "8002:8000"  # Cambia 8001 a 8002
```

### El frontend no se conecta a la API
Verifica que:
1. La API estÃ© corriendo en http://localhost:8001
2. El frontend estÃ© accediendo a la URL correcta (ver `frontend/app.js`)

## â˜ï¸ Despliegue en Render

El proyecto incluye configuraciÃ³n para desplegar en Render. Si el servicio ya existe y no detecta el `render.yaml`, configura manualmente:

### âš ï¸ ConfiguraciÃ³n Manual en Render (Recomendado si el servicio ya existe)

Si tu servicio en Render ya fue creado manualmente, ve a **Settings** y configura:

1. **Environment**: `Python 3`

2. **Build Command**:
   ```bash
   pip install --upgrade pip && pip install -r requirements.txt
   ```

3. **Start Command**:
   ```bash
   mkdir -p data/models data/raw data/interim data/processed && export PYTHONPATH="${PYTHONPATH}:$(pwd)/src" && uvicorn runner_air_planner.api.main:app --host 0.0.0.0 --port $PORT
   ```

4. **Health Check Path**: `/api/health`

5. **Environment Variables** (opcional, en la secciÃ³n Environment):
   - `PYTHONUNBUFFERED`: `1`
   - `PYTHONPATH`: `/opt/render/project/src`

### ConfiguraciÃ³n automÃ¡tica (nuevo servicio)

Si creas un **nuevo servicio** desde cero:

1. Conecta tu repositorio de GitHub a Render
2. Render deberÃ­a detectar automÃ¡ticamente el archivo `render.yaml`
3. Si no lo detecta, usa la configuraciÃ³n manual de arriba

### SoluciÃ³n de problemas comunes

**Error: "Empty build command"**
- Ve a Settings â†’ Build Command y asegÃºrate de que estÃ© configurado
- Usa el Build Command de arriba

**Error: "Publish directory build does not exist"**
- Esto significa que Render estÃ¡ tratando tu servicio como "Static Site"
- AsegÃºrate de que el tipo de servicio sea **"Web Service"** (no "Static Site")
- Ve a Settings y verifica que el tipo sea correcto

**Error: "Module not found"**
- AÃ±ade la variable de entorno `PYTHONPATH` con valor `/opt/render/project/src`
- O usa el Start Command completo de arriba que incluye el export

### Notas importantes para Render

- Render usa la variable de entorno `PORT` automÃ¡ticamente (no la definas manualmente)
- Los datos se almacenan en el sistema de archivos del servicio (no persisten entre reinicios)
- Para datos persistentes, considera usar un servicio de base de datos o almacenamiento externo
- El frontend estÃ¡tico necesita desplegarse por separado o integrarse con la API
